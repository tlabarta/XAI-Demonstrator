{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1: Training a model\n",
    "---\n",
    "\n",
    "The following code downloads the [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database), defines a model with one hidden linear layer, and trains the model by backpropagation, trying to minimize the cross-entropy loss between predictions and actual targets. Model parameters trained with PyTorch can also be [saved and loaded](https://pytorch.org/tutorials/beginner/saving_loading_models.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T22:24:34.158705Z",
     "start_time": "2023-11-10T22:24:04.489717Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the Model:\n",
      "├─ Epoch 1 loss: 0.46385365882630286 (Acc.: 91.11 %)\n",
      "├─ Epoch 2 loss: 0.31849163339368064 (Acc.: 91.43 %)\n",
      "├─ Epoch 3 loss: 0.30810985710225636 (Acc.: 91.95 %)\n",
      "├─ Epoch 4 loss: 0.29765272690161965 (Acc.: 91.52 %)\n",
      "├─ Epoch 5 loss: 0.29345519051178176 (Acc.: 90.85 %)\n",
      "├─ Epoch 6 loss: 0.2902239756638816 (Acc.: 91.71 %)\n",
      "├─ Epoch 7 loss: 0.2873591946989997 (Acc.: 91.98 %)\n",
      "├─ Epoch 8 loss: 0.2856419786000684 (Acc.: 91.97 %)\n",
      "├─ Epoch 9 loss: 0.28180348226574187 (Acc.: 91.49 %)\n",
      "├─ Epoch 10 loss: 0.28235343948149605 (Acc.: 92.15 %)\n",
      "╰─[Accuracy of the model on the test images: 92.15 %]\n"
     ]
    }
   ],
   "source": [
    "# Training a model on MNIST:\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim \n",
    "from torchvision import datasets, transforms # Define dataset \n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]) \n",
    "trainset = datasets.MNIST('./data', download=True, train=True, transform=transform) \n",
    "\n",
    "testset = datasets.MNIST('./data', download=True, train=False, transform=transform) \n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True) \n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True) \n",
    "\n",
    "\n",
    "# Define model architecture \n",
    "class Net(nn.Module):\n",
    "    def __init__(self,n=16):\n",
    "        super().__init__()\n",
    "        self.hidden_layer = nn.Linear(784,n)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.finale_layer = nn.Linear(n,10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.hidden_layer(x)\n",
    "        x = self.finale_layer(x)\n",
    "        return x\n",
    "n = 16\n",
    "model = Net(n)\n",
    "# Define loss and optimizer \n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) \n",
    "\n",
    "def evaluate(model,testloader):\n",
    "    # Test the model \n",
    "    correct = 0 \n",
    "    total = 0 \n",
    "    with torch.no_grad(): \n",
    "        for data in testloader: \n",
    "            images, labels = data \n",
    "            outputs = model(images) \n",
    "            _, predicted = torch.max(outputs.data, 1) \n",
    "            total += labels.size(0) \n",
    "            correct += (predicted == labels).sum().item() \n",
    "    return 100 * correct / total\n",
    "\n",
    "# Train the model \n",
    "print(\"Training the Model:\")\n",
    "for epoch in range(10): \n",
    "    running_loss = 0.0 \n",
    "    for i, data in enumerate(trainloader): \n",
    "        inputs, labels = data \n",
    "        optimizer.zero_grad() \n",
    "        # Forward pass \n",
    "        outputs = model(inputs) \n",
    "        loss = criterion(outputs, labels) # Backward pass and optimize \n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "        running_loss += loss.item() \n",
    "    print(f'├─ Epoch {epoch+1} loss: {running_loss/len(trainloader)} (Acc.: {evaluate(model,testloader)} %)') \n",
    "\n",
    "\n",
    "print(f'╰─[Accuracy of the model on the test images: {evaluate(model,testloader)} %]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T22:29:01.874184Z",
     "start_time": "2023-11-10T22:29:01.857508Z"
    }
   },
   "source": [
    "# Example 2: Computing a heatmap explanation with CRP\n",
    "\n",
    "---\n",
    "\n",
    "The following code computes the explanation of the network prediction in form of a heatmap. <br>\n",
    "You can use/adapt it for you backend-logic. Interactions with your web-app should change the parameters \n",
    "- `idx` - which changes the input of the neural network\n",
    "- `hidden_neuron_idx` - index of the hidden neuron (which concept) we condition the explanation on (see CRP)\n",
    "- `output_neuron_idx` - index of the output neuron (which output-class) we condition the explanation on"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load successful\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:30:00.519387Z",
     "start_time": "2023-11-10T19:30:00.436742Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Image Path: frontend/website_img/input.png\n",
      "Heatmap Image Path: frontend/website_img/heatmap.png\n"
     ]
    }
   ],
   "source": [
    "# Computing CRP heatmap explanations:\n",
    "\n",
    "# modules used for visualizing the images\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "# modules for computing the explanations\n",
    "from zennit.composites import EpsilonPlus\n",
    "from crp.attribution import CondAttribution\n",
    "\n",
    "\n",
    "vis_testset = datasets.MNIST('./data', \n",
    "                             download=True, \n",
    "                             train=False, \n",
    "                             transform=transforms.Compose([transforms.ToTensor()])) \n",
    "# ╰─ Visualization-Dataset without normalization applied.\n",
    "\n",
    "\n",
    "# ··········· PARAMETERS ·············\n",
    "# PLAY AROUND WITH THOSE VALUES TO SEE\n",
    "# HOW THE EXPLANATIONS CHANGE. THESE \n",
    "# PARAMETERS HAVE TO BE INTERACTIVELY\n",
    "# ADJUSTED BY THE USER IN YOUR DEMO.\n",
    "idx = 19                                # index of the data sample in the dataset \n",
    "hidden_neuron_idx = 8 # 0-15 or None    # index of the hidden neuron (which concept) we condition the explanation on\n",
    "output_neuron_idx = 7 # 0-9             # index of the output neuron (which output-class) we condition the explanation on\n",
    "# ····································\n",
    "\n",
    "\n",
    "# There are several different rules to compute relevances.\n",
    "# Here we initialize the explainer with our model and the LRP-rule,\n",
    "# and set the conditions for the explanation based on the above values.\n",
    "attributor = CondAttribution(model, EpsilonPlus()) \n",
    "conditions=[{\n",
    "    'y':output_neuron_idx,\n",
    "    'hidden_layer':hidden_neuron_idx\n",
    "}]\n",
    "\n",
    "# load and init sample \n",
    "sample = testset[idx][0]        # transformed test-data is used\n",
    "sample = sample.unsqueeze(0)    \n",
    "sample.requires_grad_()         # for the heatmap computation the data requires gradients\n",
    "\n",
    "# Compute the explanation:\n",
    "attribution = attributor(sample,conditions=conditions)\n",
    "heatmap = attribution.heatmap # THIS IS THE HEATMAP YOU NEED TO SHOW\n",
    "\n",
    "\n",
    "# Visualizing samples side-by-side, just for illustration purposes:\n",
    "vis_heatmap = heatmap.permute(1,2,0)\n",
    "viz_sample = vis_testset[idx][0].permute(1,2,0)\n",
    "\n",
    "fig, axes = plt.subplots(1,2,dpi=600,figsize=(8,5))\n",
    "v = max(abs(heatmap.max().item()), abs(heatmap.min().item()))\n",
    "\n",
    "axes[0].imshow(viz_sample, cmap='gray')\n",
    "axes[1].imshow(vis_heatmap, cmap='seismic',norm=Normalize(vmin=-v,vmax=v))\n",
    "\n",
    "axes[0].set_title(\"Input\")\n",
    "axes[1].set_title(\"(Conditional) Heatmap\")\n",
    "axes[0].axis('off')\n",
    "axes[1].axis('off')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
